---
title: Part 2 - Testing code
author: Dimitri Merejkowsky
date: EFREI 2022-2023
---

# Production code, test code

Production code: code that is used by the final user (more or less directly)

\vfill

Test code: code that is not used by the final user, but rather by *the authors*
of the production code.

In theory, test code should not (or must not) be shipped.

Test code *runs* the production code and *checks* it works

##

Example:

```java
/* Production code */
class Calculator {
    public static int add4(int x) {
        return x + 4;
    }
}
```

\vfill


```java
/* Test code */
class CalculatorTests {
    @Test
    public void add_four_to_one_returns_five() {
        int actual = Calculator.add4(1);
        int expected = 5;
        assertEquals(expected, actual);
    }
}
```

## The two hard drives

Though experiment:

* One hard drive with all the production code
* One hard drive with all the test code

We also assume the test suite is *complete* (it
checks all the requirements are met). But you don't
have a complete spec at hard.

Which hard drive loss would be the most damaging?


## Answer

It's better to lose the *production code* (bear with me)

\vfill


* a practical reason
* a theoretical reason

## Practical reason

It's possible (and actually not that hard) to write the production code
by making the tests pass one by one. You'll know you're done when all
the tests pass, and you'll know the new code still meets the
requirements.

It's *very* hard to guess which tests to write just by reading the production code.

## Example

Let there be a function like this in the production code:

```java
public int foo4(int x) {
    return x + 3;
}
```

Is this a bug? Is the function badly named?


\vfill

Let there by a test:

```python
@Test
void foo_4_of_3_is_6() {
    int actual = foo4(3);
    assertEquals(6, actual);
}
```
Ok, so it's not a bug

## The two values of code

Primary value: what the code *does*: `add4` allow to perform an addition

Secondary value: the fact the code can *change*. For instance, the client may
need a `add5` function in the future.

## The secondary value is the most important

The whole point of *soft*ware is that it's supposed to be easier to change
than *hard*ware.

Which means:

* The code you write is expected to be easy to change
* Even so, you'll live in a words where *everything changes all the time*

## Back to production and test code

The primary value is just assured by a "snapshot" of the production code

The secondary value is assured by:

* Quality of the production
* Quality of the tests

\vfill

That's why we rather loose the hard drive with the production code.


# Test code analysis

* Often in a different package or source file
* Each test has a *name* and some *steps*
* Test case: a collection of tests
* Test suite: a collection of test case

## Example

![](img/test-suites.png)

## Grouping

In Java, test suites are packages, test cases are classes and test are
methods.

But you'll find the same kind of grouping in other languages and frameworks.

## Anatomy of a test function

* A name
* A body

## Anatomy of a test function

```java
@Test
// ↓ Name
void can_book_one_seat_when_train_is_empty() {
  // Arrange
  var train = Helpers.makeEmptyTrain(100);
  var booking = new Booking(train);

  // Act
  var seatNumber = booking.book(1);

  // Assert
  assertTrue(train.getSeat(seatNumber).isOccupied());
}
```

## Anatomy of a test function (javascript)

```javascript
// ↓ description group
describe("booking", () => {
     // ↓ test name
  it("can book one seat from empty train", () => {
  // Arrange
  const train = Helpers.makeEmptyTrain(100);
  const booking = new Booking(train);

  // Act
  const seatNumber = booking.book(1);

  // Assert
  expect(train.getSeat(seatNumber).isOccupied())
    .to.beTrue();

  });
});
```

## Test names

Very important because they'll show up in the output.

Should be descriptive.

Usually start with `should`, `when`,  it_, ...

## Body

* Arrange / act / assert - or  given / when / then

* Try to keep one *logic* assertion by test

## Assertions

* Pay attention to the error messages

* Using a framework can help you write better assertions

## Frameworks

* Java:
  * jupiter
  * assertj

* JavaScript
  *  tape.js (underrated, but refreshingly simple)
  *  mochai, chai, jasmine, ...

## Test failure

Ideally a test fails when there's a bug in the production code:

```java
public static int add4(int x) {
    return x + 3; // <- bug here
}
```

\vfill

```text
test_add_four_to_one_returns_five failed:
    actual: 4
  expected: 5
```

## Test runs

In the simplest form, you just run all the tests
and show which failed:

```text
  train reservation
    ticket office
      ok can call ping
      FAIL context loads
      ok can reserve four seats from empty train
    train data
      ok can reset existing train
      ok can make reservation

  4 passing (23ms)
  1 failing
```

## Useful run features

* Run only one group
* Run only one test
* Re-run only the failed tests
* ...

# Continuous Integration

*Continuous Integration*, or CI for short, is any situation where there
exist *scripts* running on a *build farm*.

## CI Scripts

The *scripts* are usually written in an "scripting language" Python, Bash, PowerShell
(hence
there name).

They have access to some or all the source code and can do many things:
compiling, running some tests, produce deliveries, deploy new servers,
and so on.

They can:

  * Run automatically upon certain events (like a merge request being created)
  * Run automatically on specific hours (for instance, everyday at 9 AM)
  * Be triggered by a manual action (like running a separate program or using a Web interface).


## Build Farm

A *build farm* is made of two things. A set of *runners*, and a *coordinator*.

There may be just one runner, and the coordinator may be running on the
same machine as the runner, it does not matter; what matters is that
runners and coordinators do different things.

## Runner

A runner is any machine on which the scripts run. That's all.

## Coordinator

A coordinator:

  * Listens to the *triggers* listed above,
  * Dispatches the execution of the scripts on one or several machines among the runners,
  * Aggregates the results of those executions. Those can be test reports, logs, files, etc. ...

Note that if you're using Jenkins, the coordinator is called *master*, and the runners are called *nodes* or *slaves*.

## Job

A *job* is a script plus a *configuration*. The configuration contains
at least the specifications for the triggers, but may also contain
mappings of keys and values.

## Build

A *build* is the execution of a job.

Usually, builds have a *number*, and jobs have a *name*.

P.S: I know that "build" has many other meanings in the programming world, but I don't have a better one.

## Failure and success

Builds have **only one way** to *succeed*, and many ways to *fail*. (The
script could not run, the compilation failed, all the tests but one
passed, etc. ).

By extension, we say that a job is *failing* when the most recent build
failed. We say that the job is *stable* if the last build succeed. Thus
a job can *go from stable to failing*, or *be back to stable*.

Jobs can have other states depending on the coordinator such as
"skipped", "starting", or "canceled".

Side note: Jenkins assigns colors to builds, such as "yellow" when the
compilation is OK but some tests are failing. For me a yellow build is
just an other kind of *failed* build.

## Notification

When a job reaches a certain state, you may use a *notification* to
communicate the event to some humans.

For instance, if a build triggered by a pull request fails, an e-mail
may be sent by the coordinator to its author.


## Step

We saw that scripts can do many various tasks. Scripts often go through
a series of *steps*,

* Fetch the code
* Compile everything
* Run the tests
* (Optionally - Generate an archive)

Usually a failure in one of the steps causes the script to terminate
immediately (but not always).

Also, the series of steps may differ from one job to an other. For
instance, you may want the "deploy to production" step to only run for
certain triggers :P

## Artifact

An *artifact* is a file produced by one or several steps. Artifacts may
be exchanged across jobs, in some cases *via* the coordinator, but other
mechanisms may be used.

## Delivery

A *delivery* is an artifact that can be used by your customers.
Deliveries are often deployed to a download page.

## Deployment

A *deployment* is a special kind of job that consists in either:

* Uploading some files to a remote location
* Updating some code that is running somewhere else

In the second case you may:

* Upload the source files directly to the server. You may do this for sites written in PHP for instance.
* Run some Ansible Playbooks
* Build and upload a Docker image to a container registry and then ask some nodes in a cluster to use the new docker image
* ... or any other technique

Since deployments are just jobs, they *also* have only one way to succeed and many ways to fail ...;

## Environment

An environment is just a group of machines where a certain kind deployment occurs.

For instance, you may decide that:

* Every deployment triggered by a push on the master branch will go to an environment called "pre-production"
* Every deployment triggered by a tag starting with 'v' will go to an environment called "production".

By the way, GitLab CI has an [excellent support for
environments](https://docs.gitlab.com/ee/ci/environments.html).

## Recap

![](img/ci.png)

## Examples

* Jenkins
* GitLab CI
* GitHub Actions


## Why to use CI

* Assuring cross-platform functionality
* Assuring cross-browser functionality
* Because devs will forget to run the tests!
* Recognized as a good practice

## How to use CI

My advice: run it for each PR, during the code review,
*and* after each merge on master.

Select devs in the team to babysit the build farm (rotating role)


# Coverage

Goal: see lines of code that are not hit by the tests

Works by running the whole test suite and "instrumenting" the code.

## Example

```java
int foo(boolean x, boolean y) {
    if (x) {
        return 1;
    }
    if (y) {
        return 2;
    }
    return 0;
}
```

\vfill

```java
@Test
public void testFoo() {
    int actual = foo(true, false);
    assertEquals(1, actual);
}
```

## Example

```text
Coverage for `foo()`: 5 statements, 3 missing

> void foo(boolean x, boolean y) {
>     if (x) {
>         return 1;
>     }
!     if (y) {
!         return 2;
!     }
!     return 0;
> }
```

## Limits of coverage

It's tempting to monitor the percentage of coverage but:

* If a file is not hit *at* all by the tests, it may not even appear in the report
* You have to exclude test code from the report
* A high level of coverage can give you a false sense of security

## Covering branches - 1

Let's say we have this production code:

```java
// Production code
public static Foo buildFoo(int x, int y) {
    Foo foo = new Foo();
    if (x < y) {
        foo.attribute = /* ... */;
    }
    if (y < x) {
        foo.attribute = /* ... */;
    }
    return foo;
}
```

## Covering branches - 2

And this test code:

```java
// Test code
@Test
void building_foo_when_x_is_lower_than_y() {
    var foo = Foo.buildFoo(1, 2);
    assertEquals(..., foo.attribute);
}

@Test
void building_foo_when_x_is_greater_than_y() {
    var foo = Foo.buildFoo(2, 1);
    assertEquals(..., foo.attribute);
}
```

## Covering branches - 3

The code will be 100% covered, but what happens when `x` equals `y`?

\vfill

Who knows?

```java
var foo = Foo.buildFoo(1, 1);
// foo.attribute is null!
```

## Coverage in practice

* Try to keep it around 80%

* If it becomes too hard to reach higher coverage, just don't.

* Take a look at the coverage report once in a while to
  find dead code or missing tests

# Test limitations

## Test code is not human

The final user is often a *human*. So running tests on a machine we'll never
be the same as being used by a human.

QA can get closer to that, though

## Bad tests - no assertion


```java
@Test
public void add_two_of_three_should_return_six() {
    int result = Calculator.addTwo(3);
}
```

* This test will always pass
* False sentiment of security

## Bad tests - incorrect assertion

Two wrongs don't make a right ...


```java
public int addTwo(int x)  {
    return x + 3; // <- bug here
}
```

```java
@Test
public void add_two_of_three_should_return_six() {
    int result = Calculator.addTwo(3);
    assertEquals(6, result); // <- second bug here
}
```

## False positives / silent bugs

Sometime the test will fail because there's a bug in the test code, and no bug
in the production code. This is annoying.

\vfill

Sometimes the tests will pass even though there's a bug in the production code.
This is called a *silent bug* and it's *terrifying*.


## Flakyness

Sometimes the tests will fail one every ten runs.

This is *very* annoying, but if you don't deal properly with flaky tests, it's going to get worse.

Better no test than a flaky one.

# Recap

## What do you expect from this course?

https://forms.office.com/r/LvZqbVZ21h
